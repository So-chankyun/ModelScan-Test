{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Description\n",
    "- 우선 기능 단위로 만들고 하나의 모듈(클래스)로 만들어서 작동시켜보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "from safetensors.torch import load_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convert safetensors -> pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"/Users/dataeng/modelscan-test/modelscan/unscaned/3901296/request-01/model\"\n",
    "\n",
    "ext_in = 'safetensors'\n",
    "ext_out = 'npy'\n",
    "\n",
    "MODEL_NAME = \"Mistral-Nemo-Instruct-2407\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(ext_in:str,path:str):\n",
    "    tensors = None\n",
    "\n",
    "    if ext_in == 'safetensors':\n",
    "        print(\"execute load_file\")\n",
    "        tensors = load_file(path)\n",
    "    \n",
    "    return tensors\n",
    "\n",
    "def store_model(ext_out:str,path:str,tensors=None):\n",
    "    '''\n",
    "    tensor는 어떤 library를 사용하여 load 하는지에 따라 형식이 달라질 수 있다.\n",
    "    추후 다양한 type을 지원할 수 있도록 모듈을 생성해야할 것이다.\n",
    "\n",
    "    1. torch : collections.OrderedDict\n",
    "    '''\n",
    "    if ext_out == 'pkl':\n",
    "        # pickle을 사용하여 state_dict 저장\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(tensors, f)\n",
    "\n",
    "def convert(base_path:str,model_name:str,file_name:str,ext_in:str, ext_out:str):\n",
    "    # 1. load model weight with ext_in extension file\n",
    "    load_model_file_path = os.path.join(base_path,ext_in,model_name,file_name)\n",
    "    print(f\"Source Model File Path : {load_model_file_path}\")\n",
    "\n",
    "    # 각 확장자 별로 load함수를 달리 해야한다.\n",
    "    tensors = load_model(ext_in=ext_in,path=load_model_file_path)\n",
    "\n",
    "    # 2. store model weight with ext_out extension file\n",
    "    # .pkl 파일 경로 지정\n",
    "    store_model_file_name = '.'.join([file_name.split('.')[0],ext_out])\n",
    "    store_model_file_path = os.path.join(base_path,ext_out,model_name,store_model_file_name)\n",
    "    print(f\"Store File Model File Path : {store_model_file_path}\")\n",
    "    store_model(ext_out=ext_out,path=store_model_file_path,tensors=tensors)  \n",
    "\n",
    "# def model_scan(base_path:str,model_name:str,file_name:str,ext_in:str, ext_out:str):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Approach | Popularity | Risk of Model Serialization Attack Exploitability |\n",
    "| --- | --- | --- |\n",
    "| Pickle Variants | Very high | Very high |\n",
    "| Tensorflow SavedModel | High | Medium |\n",
    "| H5 (Keras) | High | Low (except Keras Lambda layer) |\n",
    "| Inference Only | Medium | Low |\n",
    "| Vector/Tensor Only | Low | Very low |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelScan():\n",
    "    def __init__(self,base_path:str,model_name:str,ext_in:str,ext_out:str):\n",
    "        self.base_path:str = base_path\n",
    "        self.model_name:str = model_name\n",
    "        self.ext_in:str = ext_in\n",
    "        self.ext_out:str = ext_out\n",
    "        self.tensors:dict = {}\n",
    "\n",
    "    def __model_file_list(self,isLoad=True) -> list:\n",
    "        '''\n",
    "        load model file list\n",
    "        '''\n",
    "        model_weight_file_list = None\n",
    "\n",
    "        # true -> load, false -> store\n",
    "        extension = ext_in if isLoad else ext_out\n",
    "\n",
    "        model_file_list = os.listdir(os.path.join(self.base_path,extension,self.model_name))\n",
    "        model_weight_file_list = [fn for fn in model_file_list if fn.endswith('.'+extension) and not 'consolidated' in fn]\n",
    "    \n",
    "        return model_weight_file_list\n",
    "    \n",
    "    def __load_model(self,path:str):\n",
    "        '''\n",
    "        다양한 extension input을 지원할 수 있도록 해야함.\n",
    "        '''\n",
    "        tensor = None\n",
    "\n",
    "        if self.ext_in == 'safetensors':\n",
    "            print(\"execute load_file\")\n",
    "            tensor = load_file(path)\n",
    "        \n",
    "        return tensor\n",
    "    \n",
    "    def __store_model(self,path:str,tensors=None):\n",
    "        '''\n",
    "        tensor는 어떤 library를 사용하여 load 하는지에 따라 형식이 달라질 수 있다.\n",
    "        추후 다양한 type을 지원할 수 있도록 모듈을 생성해야할 것이다.\n",
    "\n",
    "        1. torch : collections.OrderedDict\n",
    "        '''\n",
    "        if self.ext_out == 'pkl':\n",
    "            # pickle을 사용하여 state_dict 저장\n",
    "            with open(path, 'wb') as f:\n",
    "                pickle.dump(tensors, f)\n",
    "        elif self.ext_out == 'npy':\n",
    "            for name, array in tensors.items():\n",
    "                np.save(f'{name}.npy', array)\n",
    "\n",
    "    def __convert(self):\n",
    "        '''\n",
    "        description:\n",
    "            Scaning All Model files(like .safetensor,,)\n",
    "            이 convert 함수를 멀티 thread 혹은 processing으로 구현하면 좋을 것 같은데...\n",
    "        return:\n",
    "\n",
    "        '''\n",
    "        # 1. load model weight with ext_in extension file\n",
    "        load_model_file_list = self.__model_file_list(isLoad=True)\n",
    "        for f in load_model_file_list:\n",
    "            load_model_file_path = os.path.join(self.base_path,self.ext_in,self.model_name,f)\n",
    "            print(f\"Source Model File Path : {load_model_file_path}\")\n",
    "\n",
    "            # 각 확장자 별로 load함수를 달리 해야한다.\n",
    "            model_weight_file_name = f.split('.')[0]\n",
    "            self.tensors[model_weight_file_name] = self.__load_model(path=load_model_file_path)\n",
    "\n",
    "        # 2. store model weight with ext_out extension file\n",
    "        # .pkl 파일 경로 지정\n",
    "        for i,f in enumerate(load_model_file_list):\n",
    "            store_model_file_name = f.split('.')[0]\n",
    "            store_model_file_path = os.path.join(self.base_path,ext_out,self.model_name,store_model_file_name+'.'+self.ext_out)\n",
    "            print(f\"Store File Model File Path : {store_model_file_path}\")\n",
    "            self.__store_model(path=store_model_file_path,tensors=self.tensors[store_model_file_name]) \n",
    "    \n",
    "    def scan(self):\n",
    "        # convert\n",
    "        # check해야 할 필요가 없는 파일은 pass 하도록 구현\n",
    "        # convert가 필요한 확장자들을 미리 정의해놓자.\n",
    "        self.__convert()\n",
    "        \n",
    "        # 명령 실행\n",
    "        store_model_folder_path = os.path.join(self.base_path,ext_out,self.model_name)\n",
    "\n",
    "        command = f\"modelscan -p {store_model_folder_path} -r json -o output-file-name.json\"\n",
    "        commands = command.split(\" \")\n",
    "        result = subprocess.run(commands, capture_output=True, text=True)\n",
    "\n",
    "        # 결과 출력\n",
    "        # print(\"Return Code:\", result.returncode)\n",
    "        # print(\"Standard Output:\", result.stdout)\n",
    "        # print(\"Standard Error:\", result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Model File Path : /Users/dataeng/modelscan-test/modelscan/unscaned/3901296/request-01/model/safetensors/Mistral-Nemo-Instruct-2407/model-00003-of-00005.safetensors\n",
      "execute load_file\n",
      "Source Model File Path : /Users/dataeng/modelscan-test/modelscan/unscaned/3901296/request-01/model/safetensors/Mistral-Nemo-Instruct-2407/model-00002-of-00005.safetensors\n",
      "execute load_file\n",
      "Source Model File Path : /Users/dataeng/modelscan-test/modelscan/unscaned/3901296/request-01/model/safetensors/Mistral-Nemo-Instruct-2407/model-00001-of-00005.safetensors\n",
      "execute load_file\n",
      "Source Model File Path : /Users/dataeng/modelscan-test/modelscan/unscaned/3901296/request-01/model/safetensors/Mistral-Nemo-Instruct-2407/model-00004-of-00005.safetensors\n",
      "execute load_file\n",
      "Source Model File Path : /Users/dataeng/modelscan-test/modelscan/unscaned/3901296/request-01/model/safetensors/Mistral-Nemo-Instruct-2407/model-00005-of-00005.safetensors\n",
      "execute load_file\n",
      "Store File Model File Path : /Users/dataeng/modelscan-test/modelscan/unscaned/3901296/request-01/model/pkl/Mistral-Nemo-Instruct-2407/model-00003-of-00005.pkl\n",
      "Store File Model File Path : /Users/dataeng/modelscan-test/modelscan/unscaned/3901296/request-01/model/pkl/Mistral-Nemo-Instruct-2407/model-00002-of-00005.pkl\n",
      "Store File Model File Path : /Users/dataeng/modelscan-test/modelscan/unscaned/3901296/request-01/model/pkl/Mistral-Nemo-Instruct-2407/model-00001-of-00005.pkl\n",
      "Store File Model File Path : /Users/dataeng/modelscan-test/modelscan/unscaned/3901296/request-01/model/pkl/Mistral-Nemo-Instruct-2407/model-00004-of-00005.pkl\n",
      "Store File Model File Path : /Users/dataeng/modelscan-test/modelscan/unscaned/3901296/request-01/model/pkl/Mistral-Nemo-Instruct-2407/model-00005-of-00005.pkl\n",
      "Return Code: 0\n",
      "Standard Output: No settings file detected at /Users/dataeng/modelscan-test/code/modelscan-settings.toml. Using defaults. \n",
      "\n",
      "Scanning /Users/dataeng/modelscan-test/modelscan/unscaned/3901296/request-01/model/pkl/Mistral-Nemo-Instruct-2407/model-00003-of-00005.pkl using modelscan.scanners.PickleUnsafeOpScan model scan\n",
      "Scanning /Users/dataeng/modelscan-test/modelscan/unscaned/3901296/request-01/model/pkl/Mistral-Nemo-Instruct-2407/model-00004-of-00005.pkl using modelscan.scanners.PickleUnsafeOpScan model scan\n",
      "Scanning /Users/dataeng/modelscan-test/modelscan/unscaned/3901296/request-01/model/pkl/Mistral-Nemo-Instruct-2407/model-00005-of-00005.pkl using modelscan.scanners.PickleUnsafeOpScan model scan\n",
      "Scanning /Users/dataeng/modelscan-test/modelscan/unscaned/3901296/request-01/model/pkl/Mistral-Nemo-Instruct-2407/model-00002-of-00005.pkl using modelscan.scanners.PickleUnsafeOpScan model scan\n",
      "Scanning /Users/dataeng/modelscan-test/modelscan/unscaned/3901296/request-01/model/pkl/Mistral-Nemo-Instruct-2407/model-00001-of-00005.pkl using modelscan.scanners.PickleUnsafeOpScan model scan\n",
      "\u001b[1m{\u001b[0m\u001b[32m\"summary\"\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m\"total_issues_by_severity\"\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m\"LOW\"\u001b[0m: \u001b[1;36m0\u001b[0m, \u001b[32m\"MEDIUM\"\u001b[0m: \u001b[1;36m0\u001b[0m, \u001b[32m\"HIGH\"\u001b[0m: \u001b[1;36m0\u001b[0m, \n",
      "\u001b[32m\"CRITICAL\"\u001b[0m: \u001b[1;36m0\u001b[0m\u001b[1m}\u001b[0m, \u001b[32m\"total_issues\"\u001b[0m: \u001b[1;36m0\u001b[0m, \u001b[32m\"input_path\"\u001b[0m: \n",
      "\u001b[32m\"/Users/dataeng/modelscan-test/modelscan/unscaned/3901296/request-01/model/pkl/M\u001b[0m\n",
      "\u001b[32mistral-Nemo-Instruct-2407\"\u001b[0m, \u001b[32m\"absolute_path\"\u001b[0m: \n",
      "\u001b[32m\"/Users/dataeng/modelscan-test/modelscan/unscaned/3901296/request-01/model/pkl/M\u001b[0m\n",
      "\u001b[32mistral-Nemo-Instruct-2407\"\u001b[0m, \u001b[32m\"modelscan_version\"\u001b[0m: \u001b[32m\"0.8.1\"\u001b[0m, \u001b[32m\"timestamp\"\u001b[0m: \n",
      "\u001b[32m\"2024-12-09T09:00:23.247919\"\u001b[0m, \u001b[32m\"scanned\"\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m\"total_scanned\"\u001b[0m: \u001b[1;36m5\u001b[0m, \u001b[32m\"scanned_files\"\u001b[0m: \n",
      "\u001b[1m[\u001b[0m\u001b[32m\"model-00003-of-00005.pkl\"\u001b[0m, \u001b[32m\"model-00004-of-00005.pkl\"\u001b[0m, \n",
      "\u001b[32m\"model-00005-of-00005.pkl\"\u001b[0m, \u001b[32m\"model-00002-of-00005.pkl\"\u001b[0m, \n",
      "\u001b[32m\"model-00001-of-00005.pkl\"\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m, \u001b[32m\"issues\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m\"errors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
      "\n",
      "Standard Error: \n"
     ]
    }
   ],
   "source": [
    "BASE_PATH = \"/Users/dataeng/modelscan-test/modelscan/unscaned/3901296/request-01/model\"\n",
    "\n",
    "ext_in = 'safetensors'\n",
    "ext_out = 'pkl'\n",
    "\n",
    "MODEL_NAME = \"Mistral-Nemo-Instruct-2407\"\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    scanner = ModelScan(\n",
    "                base_path=BASE_PATH,\n",
    "                model_name=MODEL_NAME,\n",
    "                ext_in=ext_in,\n",
    "                ext_out=ext_out\n",
    "            )\n",
    "\n",
    "    scanner.scan()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Huggingface 모델 다운로드 로직 구성\n",
    "- 시나리오는 다음과 같다.\n",
    "    1. 모델 파일 요청(특정 user, 특정 모델)\n",
    "    2. 모델 파일 탐색\n",
    "    3. 모델 파일 취약성 점검 여부 확인(protect AI Knowledge Base or Huggingface)\n",
    "    4. token 발급 or 기존 데이터 사용\n",
    "    5. 다운로드 to blob(blob설계 필요. azcopy 특성을 고려한)\n",
    "    6. p-region copy 수행(MSP의 허가가 필요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(repo_id=\"bert-base-uncased\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modelscan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
